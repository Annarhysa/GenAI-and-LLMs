{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.9","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Poem Generation using FastAI\n","metadata":{"id":"HTYTRlgEInNS"}},{"cell_type":"markdown","source":"In this tutorial you will see how to fine-tune a pretrained transformer model from the transformers library by HuggingFace. It can be very simple with FastAI's data loaders. It's possible to use any of the pretrained models from HuggingFace. Below we will experiment with GPT2. ","metadata":{"id":"mM0YJ6eu6Vkk"}},{"cell_type":"markdown","source":"## Import Libraries\n","metadata":{"id":"-FVVxuGPI2-e"}},{"cell_type":"code","source":"# from fastbook import *\nfrom fastai.text.all import *\nfrom transformers import GPT2LMHeadModel, GPT2TokenizerFast","metadata":{"id":"Jd51B8ZYIaky","execution":{"iopub.status.busy":"2023-10-14T03:15:41.764889Z","iopub.execute_input":"2023-10-14T03:15:41.765172Z","iopub.status.idle":"2023-10-14T03:15:41.769948Z","shell.execute_reply.started":"2023-10-14T03:15:41.765146Z","shell.execute_reply":"2023-10-14T03:15:41.768672Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"pretrained_weights = 'gpt2'\ntokenizer = GPT2TokenizerFast.from_pretrained(pretrained_weights)\nmodel = GPT2LMHeadModel.from_pretrained(pretrained_weights)","metadata":{"id":"alC0xd-aNjKF","execution":{"iopub.status.busy":"2023-10-14T03:15:43.684970Z","iopub.execute_input":"2023-10-14T03:15:43.685246Z","iopub.status.idle":"2023-10-14T03:16:07.709927Z","shell.execute_reply.started":"2023-10-14T03:15:43.685219Z","shell.execute_reply":"2023-10-14T03:16:07.708559Z"},"trusted":true},"execution_count":5,"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","\u001b[0;32m<ipython-input-5-d92719671d32>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mpretrained_weights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'gpt2'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mGPT2TokenizerFast\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpretrained_weights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mGPT2LMHeadModel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpretrained_weights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/transformers/tokenization_utils_base.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *init_inputs, **kwargs)\u001b[0m\n\u001b[1;32m   1743\u001b[0m                         \u001b[0mresume_download\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mresume_download\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1744\u001b[0m                         \u001b[0mlocal_files_only\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlocal_files_only\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1745\u001b[0;31m                         \u001b[0muse_auth_token\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_auth_token\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1746\u001b[0m                     )\n\u001b[1;32m   1747\u001b[0m                 \u001b[0;32mexcept\u001b[0m \u001b[0mrequests\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexceptions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mHTTPError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/transformers/file_utils.py\u001b[0m in \u001b[0;36mcached_path\u001b[0;34m(url_or_filename, cache_dir, force_download, proxies, resume_download, user_agent, extract_compressed_file, force_extract, use_auth_token, local_files_only)\u001b[0m\n\u001b[1;32m   1054\u001b[0m             \u001b[0muser_agent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muser_agent\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1055\u001b[0m             \u001b[0muse_auth_token\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_auth_token\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1056\u001b[0;31m             \u001b[0mlocal_files_only\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlocal_files_only\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1057\u001b[0m         )\n\u001b[1;32m   1058\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl_or_filename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/transformers/file_utils.py\u001b[0m in \u001b[0;36mget_from_cache\u001b[0;34m(url, cache_dir, force_download, proxies, etag_timeout, resume_download, user_agent, use_auth_token, local_files_only)\u001b[0m\n\u001b[1;32m   1233\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1234\u001b[0m                     raise ValueError(\n\u001b[0;32m-> 1235\u001b[0;31m                         \u001b[0;34m\"Connection error, and we cannot find the requested files in the cached path.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1236\u001b[0m                         \u001b[0;34m\" Please try again or make sure your Internet connection is on.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1237\u001b[0m                     )\n","\u001b[0;31mValueError\u001b[0m: Connection error, and we cannot find the requested files in the cached path. Please try again or make sure your Internet connection is on."],"ename":"ValueError","evalue":"Connection error, and we cannot find the requested files in the cached path. Please try again or make sure your Internet connection is on.","output_type":"error"}]},{"cell_type":"markdown","source":"## Read Data\nThis data is organized by folder. There are two main folders: forms (e.g. haiku, sonnet, etc.) and topics (e.g. love, peace, etc.). Those main folders contain subfolders for the subcategories and then the poem txt files are contained in those.\nWith fastai, it's quite easy to read the data with the the get_text_files function. You can select all folders or select specific ones.","metadata":{"id":"-VxqkIvvJ9gv"}},{"cell_type":"code","source":"path = '../input/poemsdataset'","metadata":{"id":"8CIpgsfMQPfu","execution":{"iopub.status.busy":"2023-10-13T02:12:56.91625Z","iopub.execute_input":"2023-10-13T02:12:56.916695Z","iopub.status.idle":"2023-10-13T02:12:56.921411Z","shell.execute_reply.started":"2023-10-13T02:12:56.916656Z","shell.execute_reply":"2023-10-13T02:12:56.92042Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"poems = get_text_files(path, folders = ['forms','topics'])\nprint(\"There are\",len(poems),\"poems in the dataset\")","metadata":{"id":"rOZu9LCfMVPZ","outputId":"9b6be92a-14cd-40e6-e0f6-720ff9c92774","execution":{"iopub.status.busy":"2023-10-13T02:13:00.8697Z","iopub.execute_input":"2023-10-13T02:13:00.870092Z","iopub.status.idle":"2023-10-13T02:13:05.318956Z","shell.execute_reply.started":"2023-10-13T02:13:00.870061Z","shell.execute_reply":"2023-10-13T02:13:05.317971Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We'll start off with training the model on ballads. There are only 100 ballads so it won't take as long to train. However you can add more poem forms. For instance, a haiku would be very cool to experiment with and to see if it maintains the 5,7,5 syllable structure. You can also change the path to the topics folder instead of poem forms and you can try out a bunch of poem topics like love, anger, depression, etc.. ","metadata":{"id":"u9REr2f9676E"}},{"cell_type":"code","source":"ballads = get_text_files(path+'/forms', folders = ['ballad'])\nprint(\"There are\",len(ballads),\"ballads in the dataset\")","metadata":{"id":"D6HsJQLtJ9P2","outputId":"c91cf59c-5e5f-4248-9aac-1dbe2ae6ab08","execution":{"iopub.status.busy":"2023-10-13T02:13:09.607921Z","iopub.execute_input":"2023-10-13T02:13:09.611448Z","iopub.status.idle":"2023-10-13T02:13:09.626533Z","shell.execute_reply.started":"2023-10-13T02:13:09.611392Z","shell.execute_reply":"2023-10-13T02:13:09.625526Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"txt = poems[0].open().read(); #read the first file\nprint(txt)","metadata":{"id":"aff_yTWDMlkm","outputId":"d59f2fff-0c13-40dc-bf09-97881ee40987","execution":{"iopub.status.busy":"2023-10-13T02:14:53.690941Z","iopub.execute_input":"2023-10-13T02:14:53.691252Z","iopub.status.idle":"2023-10-13T02:14:53.700261Z","shell.execute_reply.started":"2023-10-13T02:14:53.691226Z","shell.execute_reply":"2023-10-13T02:14:53.699406Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Prepare the Data\n\n","metadata":{"id":"AAiN7YVH8HwX"}},{"cell_type":"code","source":"ballads = L(o.open().read() for o in ballads) # to make things easy we will gather all texts in one numpy array","metadata":{"id":"b2-JJ8BGIgMU","execution":{"iopub.status.busy":"2023-10-13T02:14:58.719391Z","iopub.execute_input":"2023-10-13T02:14:58.71969Z","iopub.status.idle":"2023-10-13T02:14:58.748726Z","shell.execute_reply.started":"2023-10-13T02:14:58.719662Z","shell.execute_reply":"2023-10-13T02:14:58.747115Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def flatten(A):\n    rt = []\n    for i in A:\n        if isinstance(i,list): rt.extend(flatten(i))\n        else: rt.append(i)\n    return rt\n  \nall_ballads = flatten(ballads)","metadata":{"id":"Lu4ek-rIIn84","execution":{"iopub.status.busy":"2023-10-13T02:15:02.805235Z","iopub.execute_input":"2023-10-13T02:15:02.808011Z","iopub.status.idle":"2023-10-13T02:15:02.815542Z","shell.execute_reply.started":"2023-10-13T02:15:02.807968Z","shell.execute_reply":"2023-10-13T02:15:02.814392Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class TransformersTokenizer(Transform):\n    def __init__(self, tokenizer): self.tokenizer = tokenizer\n    def encodes(self, x): \n        toks = self.tokenizer.tokenize(x)\n        return tensor(self.tokenizer.convert_tokens_to_ids(toks))\n    def decodes(self, x): return TitledStr(self.tokenizer.decode(x.cpu().numpy()))","metadata":{"id":"XHPQpvvbabmj","execution":{"iopub.status.busy":"2023-10-13T02:15:06.084615Z","iopub.execute_input":"2023-10-13T02:15:06.08493Z","iopub.status.idle":"2023-10-13T02:15:06.090255Z","shell.execute_reply.started":"2023-10-13T02:15:06.084901Z","shell.execute_reply":"2023-10-13T02:15:06.089169Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"splits = [range_of(70), range(100)] # use a 70/30 split\ntls = TfmdLists(all_ballads, TransformersTokenizer(tokenizer), splits=splits, dl_type=LMDataLoader)","metadata":{"id":"KEdmTZR7NG5E","execution":{"iopub.status.busy":"2023-10-13T02:15:07.437614Z","iopub.execute_input":"2023-10-13T02:15:07.437941Z","iopub.status.idle":"2023-10-13T02:15:07.445489Z","shell.execute_reply.started":"2023-10-13T02:15:07.437911Z","shell.execute_reply":"2023-10-13T02:15:07.444403Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"show_at(tls.train, 0)","metadata":{"id":"0O-QWa9LN1IK","outputId":"7aecbf08-6d2e-4016-c2b5-d124d4e94329","execution":{"iopub.status.busy":"2023-10-13T02:15:15.813702Z","iopub.execute_input":"2023-10-13T02:15:15.814059Z","iopub.status.idle":"2023-10-13T02:15:20.51105Z","shell.execute_reply.started":"2023-10-13T02:15:15.814029Z","shell.execute_reply":"2023-10-13T02:15:20.510122Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"bs,sl = 4,256\ndls = tls.dataloaders(bs=bs, seq_len=sl)","metadata":{"id":"Fv2mSN4LOHob","execution":{"iopub.status.busy":"2023-10-13T02:15:32.912627Z","iopub.execute_input":"2023-10-13T02:15:32.912964Z","iopub.status.idle":"2023-10-13T02:15:39.445044Z","shell.execute_reply.started":"2023-10-13T02:15:32.912933Z","shell.execute_reply":"2023-10-13T02:15:39.444287Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dls.show_batch(max_n=2)","metadata":{"id":"xpbniW8mOT_g","outputId":"83aaae79-c35d-4192-920e-8e62869e01ec","execution":{"iopub.status.busy":"2023-10-13T02:15:43.485259Z","iopub.execute_input":"2023-10-13T02:15:43.485559Z","iopub.status.idle":"2023-10-13T02:15:43.610728Z","shell.execute_reply.started":"2023-10-13T02:15:43.485532Z","shell.execute_reply":"2023-10-13T02:15:43.609829Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Fine-tuning the model","metadata":{"id":"kMFnNw748QhY"}},{"cell_type":"code","source":"class DropOutput(Callback):\n    def after_pred(self): self.learn.pred = self.pred[0]","metadata":{"id":"jkJCLFncOcqV","execution":{"iopub.status.busy":"2023-10-13T02:15:49.270918Z","iopub.execute_input":"2023-10-13T02:15:49.271231Z","iopub.status.idle":"2023-10-13T02:15:49.275094Z","shell.execute_reply.started":"2023-10-13T02:15:49.271205Z","shell.execute_reply":"2023-10-13T02:15:49.274193Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"learn = Learner(dls, model, loss_func=CrossEntropyLossFlat(), cbs=[DropOutput], metrics=Perplexity()).to_fp16()","metadata":{"id":"669gGIAuOeJ8","execution":{"iopub.status.busy":"2023-10-13T02:15:51.911635Z","iopub.execute_input":"2023-10-13T02:15:51.911967Z","iopub.status.idle":"2023-10-13T02:15:51.918041Z","shell.execute_reply.started":"2023-10-13T02:15:51.911935Z","shell.execute_reply":"2023-10-13T02:15:51.917007Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"learn.validate()","metadata":{"id":"Z7ZPcyW8Oh0_","outputId":"ef19c779-2e66-41c4-cad8-e908400c1752","execution":{"iopub.status.busy":"2023-10-13T02:16:19.575487Z","iopub.execute_input":"2023-10-13T02:16:19.575784Z","iopub.status.idle":"2023-10-13T02:16:35.568428Z","shell.execute_reply.started":"2023-10-13T02:16:19.575755Z","shell.execute_reply":"2023-10-13T02:16:35.56752Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"learn.lr_find()","metadata":{"id":"ykYFFlCBOnqp","outputId":"27dae545-3669-4386-d399-76a8da0f964f","execution":{"iopub.status.busy":"2023-10-13T02:16:41.988528Z","iopub.execute_input":"2023-10-13T02:16:41.988836Z","iopub.status.idle":"2023-10-13T02:17:14.710775Z","shell.execute_reply.started":"2023-10-13T02:16:41.988809Z","shell.execute_reply":"2023-10-13T02:17:14.709945Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"learn.fit_one_cycle(1, 1e-4)","metadata":{"id":"PKySv6QiOvom","outputId":"ac533474-0ec1-4ddd-910c-c47e93098c96","execution":{"iopub.status.busy":"2023-10-13T02:17:26.629781Z","iopub.execute_input":"2023-10-13T02:17:26.630142Z","iopub.status.idle":"2023-10-13T02:18:04.239768Z","shell.execute_reply.started":"2023-10-13T02:17:26.630108Z","shell.execute_reply":"2023-10-13T02:18:04.238912Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Poem Generation Example","metadata":{"id":"fdLpyolz8bV7"}},{"cell_type":"code","source":"prompt = 'love is ridiculous' # create an initial text prompt to start your generated text\nprompt_ids = tokenizer.encode(prompt)\ninp = tensor(prompt_ids)[None].cuda()\ninp.shape","metadata":{"id":"D0zOA5wXPDEv","outputId":"27f4d6c1-e3af-4416-99cc-a071130a81c7","execution":{"iopub.status.busy":"2023-10-13T02:18:15.739493Z","iopub.execute_input":"2023-10-13T02:18:15.739804Z","iopub.status.idle":"2023-10-13T02:18:15.746675Z","shell.execute_reply.started":"2023-10-13T02:18:15.739768Z","shell.execute_reply":"2023-10-13T02:18:15.745495Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Adding the `num_beams` and `no_repeat_ngram_size` arguments make a huge difference. This can be explained [here](https://huggingface.co/blog/how-to-generate). Basically beam search reduces the risk of missing hidden high probability word sequences by keeping the most likely num_beams of hypotheses at each time step and eventually choosing the hypothesis that has the overall highest probability. Without beam search you will obtain a more greedy search. Beam search will always find an output sequence with higher probability than greedy search, but is not guaranteed to find the most likely output. Moreover, without the `no_repeat_ngram_size` you will likely obtain a repeated output. Thus we add a penalty that makes sure that no n-gram appears twice by manually setting the probability of next words that could create an already seen n-gram to 0.","metadata":{"id":"_ONkBFg482US"}},{"cell_type":"code","source":"preds = learn.model.generate(inp, max_length=60, num_beams=5, no_repeat_ngram_size=2, early_stopping=True)\nprint(\"Output:\\n\" + 100 * '-')\nprint(tokenizer.decode(preds[0].cpu().numpy(), skip_special_tokens=True))","metadata":{"id":"V39qs4aX4_3B","outputId":"31c2beac-22e8-4989-8826-fc50125b80e5","execution":{"iopub.status.busy":"2023-10-13T02:18:35.882433Z","iopub.execute_input":"2023-10-13T02:18:35.882744Z","iopub.status.idle":"2023-10-13T02:18:36.745433Z","shell.execute_reply.started":"2023-10-13T02:18:35.882715Z","shell.execute_reply":"2023-10-13T02:18:36.744486Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"prompt = \"I don't know what I would do\"\nprompt_ids = tokenizer.encode(prompt)\ninp = tensor(prompt_ids)[None].cuda()\npreds = learn.model.generate(inp, max_length=60, num_beams=5, no_repeat_ngram_size=2, early_stopping=True)\nprint(\"Output:\\n\" + 100 * '-')\nprint(tokenizer.decode(preds[0].cpu().numpy(), skip_special_tokens=True))","metadata":{"id":"gEeoQrHrPb8c","outputId":"9d54d66a-654d-40c5-d87a-c9774e2c4ce0","execution":{"iopub.status.busy":"2023-10-13T02:18:51.245715Z","iopub.execute_input":"2023-10-13T02:18:51.246072Z","iopub.status.idle":"2023-10-13T02:18:52.070553Z","shell.execute_reply.started":"2023-10-13T02:18:51.246042Z","shell.execute_reply":"2023-10-13T02:18:52.069651Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}