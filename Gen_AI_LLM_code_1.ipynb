{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "G1c42fl7aIyp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "09046e2a-f654-4a64-ae0a-b2ae45ce72af"
      },
      "source": [
        "!pip install torch==1.4.0"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[31mERROR: Could not find a version that satisfies the requirement torch==1.4.0 (from versions: 1.11.0, 1.12.0, 1.12.1, 1.13.0, 1.13.1, 2.0.0, 2.0.1, 2.1.0)\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[31mERROR: No matching distribution found for torch==1.4.0\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kinYD3010vdt"
      },
      "source": [
        "import math\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.nn import Parameter\n",
        "\n",
        "# Check whether GPU is available and can be used\n",
        "# if CUDA is found then device is set accordingly\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "if not torch.cuda.is_available():\n",
        "    print(\"Consider changing your run-time to GPU or training will be slow.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "stneSw5L77Ln"
      },
      "source": [
        "## The data: Shakespeare's sonnets\n",
        "\n",
        "Shakespeare's sonnets can be found at the following URL featuring all of his works: http://shakespeare.mit.edu/\n",
        "\n",
        "For convenience reasons we have extracted all the plain text of the sonnets: https://ocw.mit.edu/ans7870/6/6.006/s08/lecturenotes/files/t8.shakespeare.txt into a separate textfile and have added it to the class' repository. We will thus download it from there:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eTxp46sNyKNQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "830a4fdd-5f05-4cd2-dacc-93423f2745e5"
      },
      "source": [
        "!wget https://raw.githubusercontent.com/ccc-frankfurt/Practical_ML_SS21/master/week06/sonnets.txt"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-10-13 08:54:28--  https://raw.githubusercontent.com/ccc-frankfurt/Practical_ML_SS21/master/week06/sonnets.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.109.133, 185.199.110.133, 185.199.111.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.109.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 94081 (92K) [text/plain]\n",
            "Saving to: ‘sonnets.txt’\n",
            "\n",
            "sonnets.txt         100%[===================>]  91.88K  --.-KB/s    in 0.03s   \n",
            "\n",
            "2023-10-13 08:54:28 (3.10 MB/s) - ‘sonnets.txt’ saved [94081/94081]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wzimDdxq8oXk"
      },
      "source": [
        "We can open the text file and print an excerpt."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fdSreQlxy11g",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3c94e36d-1f57-4feb-bd05-d1f70a94ab2f"
      },
      "source": [
        "# Open shakespeare text file and read the data\n",
        "with open('sonnets.txt', 'r') as f:\n",
        "    text = f.read()\n",
        "\n",
        "# print an excerpt of the text\n",
        "print(text[:200])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "From fairest creatures we desire increase,\n",
            "That thereby beauty's rose might never die,\n",
            "But as the riper should by time decease,\n",
            "His tender heir might bear his memory:\n",
            "But thou contracted to thine own \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CqofvzDN8u6Z"
      },
      "source": [
        "As we are interested in a character based neural network, we will now create a mapping from the characters to numbers so that we can do our matrix calculations with numerical data. One such way is to simply replace every character with the corresponding integer in an alphabetical sequence. If we print our excerpt, we can now see the corresponding numerical values of each character."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rS8C0ndK0tce",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dbe40659-4ba4-42d2-cc7b-694bad592543"
      },
      "source": [
        "# We create two dictionaries:\n",
        "# 1. int2char, which maps integers to characters\n",
        "# 2. char2int, which maps characters to integers\n",
        "chars = tuple(set(text))\n",
        "int2char = dict(enumerate(chars))\n",
        "char2int = {ch: ii for ii, ch in int2char.items()}\n",
        "\n",
        "# Encode the text\n",
        "encoded = np.array([char2int[ch] for ch in text])\n",
        "\n",
        "# Again showing the excerpt, but this time as integers\n",
        "encoded[:200]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([32, 60, 39, 37, 53, 15, 22, 56, 60, 16, 20, 23, 53, 57, 60, 16, 22,\n",
              "       23, 11, 60, 16, 20, 53, 27, 16, 53,  0, 16, 20, 56, 60, 16, 53, 56,\n",
              "       59, 57, 60, 16, 22, 20, 16, 40, 45, 25, 26, 22, 23, 53, 23, 26, 16,\n",
              "       60, 16, 44, 35, 53, 44, 16, 22, 11, 23, 35, 58, 20, 53, 60, 39, 20,\n",
              "       16, 53, 37, 56, 30, 26, 23, 53, 59, 16, 36, 16, 60, 53,  0, 56, 16,\n",
              "       40, 45, 28, 11, 23, 53, 22, 20, 53, 23, 26, 16, 53, 60, 56,  4, 16,\n",
              "       60, 53, 20, 26, 39, 11, 38,  0, 53, 44, 35, 53, 23, 56, 37, 16, 53,\n",
              "        0, 16, 57, 16, 22, 20, 16, 40, 45,  1, 56, 20, 53, 23, 16, 59,  0,\n",
              "       16, 60, 53, 26, 16, 56, 60, 53, 37, 56, 30, 26, 23, 53, 44, 16, 22,\n",
              "       60, 53, 26, 56, 20, 53, 37, 16, 37, 39, 60, 35,  3, 45, 28, 11, 23,\n",
              "       53, 23, 26, 39, 11, 53, 57, 39, 59, 23, 60, 22, 57, 23, 16,  0, 53,\n",
              "       23, 39, 53, 23, 26, 56, 59, 16, 53, 39, 27, 59, 53])"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dZCdTzB39dNl"
      },
      "source": [
        "### Data loader: batching\n",
        "\n",
        "We now have our entire text file encoded as integers, which serves as our dataset. Next, we will need to define our data loader, mainly the part that is missing, the random sampling of batches. Let us define this method:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZWq80TVX1DNo"
      },
      "source": [
        "# Defining method to make mini-batches for training\n",
        "def get_batches(arr, batch_size, seq_length):\n",
        "    # determine the flattened batch size, i.e. sequence length times batch size\n",
        "    batch_size_total = batch_size * seq_length\n",
        "    # total number of batches we can make\n",
        "    n_batches = len(arr)//batch_size_total\n",
        "\n",
        "    # Keep only enough characters to make full batches\n",
        "    arr = arr[:n_batches * batch_size_total]\n",
        "    # Reshape into batch_size rows\n",
        "    arr = arr.reshape((batch_size, -1))\n",
        "\n",
        "    # iterate through the array, one sequence at a time\n",
        "    for n in range(0, arr.shape[1], seq_length):\n",
        "        # The features\n",
        "        x = arr[:, n:n+seq_length]\n",
        "        # The targets\n",
        "        y = np.zeros_like(x)\n",
        "        try:\n",
        "            y[:, :-1], y[:, -1] = x[:, 1:], arr[:, n+seq_length]\n",
        "        except IndexError:\n",
        "            y[:, :-1], y[:, -1] = x[:, 1:], arr[:, 0]\n",
        "        yield x, y"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FWDjvcQW92ze"
      },
      "source": [
        "### Targets/Labels\n",
        "\n",
        "We will be treating our problem as a classification task, where given an input the task is to predict the likelihood of the next character, i.e. we choose the class/character with the highest probability of a SoftMax output. Our model's output is thus a vector containing a probability for each unique character.\n",
        "\n",
        "Since we want to be able to feed our model's output back as input for the next time step, we should also give the network a one-hot encoded character as the input instead of just an integer, similar to what we have seen on our lecture's last slide.\n",
        "This way the network gets as input a one-hot vector of length corresponding to the number of total unique characters and predicts the likelihood for each character as output (for the next character in the sequence). We will thus write a function that converts our encoded characters from integers to one-hot vectors."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6l9QohlqnU-B"
      },
      "source": [
        "def one_hot_encode(arr, n_labels):\n",
        "\n",
        "    # Initialize the the encoded array\n",
        "    one_hot = np.zeros((np.multiply(*arr.shape), n_labels), dtype=np.float32)\n",
        "\n",
        "    # Fill the appropriate elements with ones\n",
        "    one_hot[np.arange(one_hot.shape[0]), arr.flatten()] = 1.\n",
        "\n",
        "    # Finally reshape it to get back to the original array\n",
        "    one_hot = one_hot.reshape((*arr.shape, n_labels))\n",
        "\n",
        "    return one_hot"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0GHm8hBaALrD"
      },
      "source": [
        "## A simple RNN\n",
        "\n",
        "We will start with writing a simple RNN in PyTorch. To get a better understanding of how the RNN model works, we will not be using PyTorch's convenience RNN implementation, but write the main portion by hand ourselves. We will later use the convenience functions for the much more complicated LSTMs.\n",
        "\n",
        "Note that we could in principle do the same thing in pure Numpy but the advantage of implementing the forward logic in PyTorch is that we can use the automatic differentation for our backward pass and we do not need to implement the backpropagation through time ourselves.\n",
        "\n",
        "What we will learn here is:\n",
        "1. How to write a recurrent neural network (the forward pass)\n",
        "2. How to implement custom mathematical equations in the forward pass of a PyTorch model and leverage the automatic backward."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cgkv_TmovgBY"
      },
      "source": [
        "class RNN(nn.Module):\n",
        "    def __init__(self, chars, device, hidden_sz, drop_prob=0.5):\n",
        "        super().__init__()\n",
        "\n",
        "        self.device = device\n",
        "\n",
        "        # creating character dictionaries\n",
        "        # we already have this code on the top, but giving it to our model\n",
        "        # will be convenient for doing predictions later\n",
        "        # i.e. doing conversions from text to integers to one-hot & vice-versa\n",
        "        self.n_chars = len(chars)\n",
        "        self.int2char = dict(enumerate(chars))\n",
        "        self.char2int = {ch: ii for ii, ch in self.int2char.items()}\n",
        "\n",
        "        self.hidden_sz = hidden_sz\n",
        "\n",
        "        # Note that this class inherits from the torch neural network class\n",
        "        # Instead of using a pre-built function we will write the math ourselves\n",
        "        # For this reason we will first need to define \"Parameters()\", that\n",
        "        # the PyTorch graph keeps track of and can optimize. In other words,\n",
        "        # let's give our class the weights & the bias that the RNN will need.\n",
        "        self.weight_ih = Parameter(torch.Tensor(self.n_chars, self.hidden_sz))\n",
        "        self.weight_hh = Parameter(torch.Tensor(self.hidden_sz, self.hidden_sz))\n",
        "        self.bias_hh = Parameter(torch.Tensor(self.hidden_sz))\n",
        "\n",
        "        # Now that we have defined the RNN cell, let us define the output layer\n",
        "        # We will use a dropout layer to prevent overfitting and then\n",
        "        # follow with a conventional linear layer (matrix multiplication) that\n",
        "        # maps the RNN cell's output (the hidden state of the network) to the\n",
        "        # class output. Remembert that the class output corresponds to a\n",
        "        # vector of length of unique characters.\n",
        "\n",
        "        # define a dropout layer\n",
        "        self.dropout = nn.Dropout(drop_prob)\n",
        "\n",
        "        # define the final, fully-connected output layer. We can use a\n",
        "        # PyTorch nn function here (or you could add the corresponding math\n",
        "        # below and assign an additional weight & bias at the top).\n",
        "        # We can see that we can create very custom models this way\n",
        "        self.fc = nn.Linear(self.hidden_sz, self.n_chars)\n",
        "\n",
        "        # We have assigned the Parameters above, but we will need to also\n",
        "        # initialize them. Let's write a function for that and initialize\n",
        "        # our weights and bias.\n",
        "        self.init_weights()\n",
        "\n",
        "    def init_weights(self):\n",
        "        nn.init.xavier_uniform_(self.weight_ih)\n",
        "        nn.init.xavier_uniform_(self.weight_hh)\n",
        "        nn.init.zeros_(self.bias_hh)\n",
        "\n",
        "    def forward(self, x, h_t):\n",
        "        \"\"\"Assumes x is of shape (batch, sequence, feature)\"\"\"\n",
        "        bs, seq_sz, _ = x.size()\n",
        "        hidden_seq = []\n",
        "\n",
        "        # Given an input and an initial hidden state, calculate the next hidden\n",
        "        # state for each sequence element.\n",
        "        # We append all the hidden states to a list (similar to a batch size)\n",
        "        # so that we can concatenate them in the batch and feed them to our\n",
        "        # last linear layer all in parallel to avoid looping through the final\n",
        "        # output layer as there is no more dependence on other time steps.\n",
        "        for t in range(seq_sz):\n",
        "            x_t = x[:, t, :]\n",
        "            h_t = torch.tanh(x_t @ self.weight_ih + h_t @ self.weight_hh + self.bias_hh)\n",
        "            hidden_seq.append(h_t.unsqueeze(0))\n",
        "\n",
        "        # Do the concatenation and reshaping for convenience\n",
        "        hidden_seq = torch.cat(hidden_seq, dim=0)\n",
        "        # reshape from shape (sequence, batch, feature) to (batch, sequence, feature)\n",
        "        hidden_seq = hidden_seq.transpose(0, 1).contiguous()\n",
        "\n",
        "        # Stack up the RNN outputs using view so that we can process the last\n",
        "        # layer in parallel\n",
        "        r_output = hidden_seq.contiguous().view(-1, self.hidden_sz)\n",
        "\n",
        "        # pass through a dropout layer\n",
        "        out = self.dropout(r_output)\n",
        "\n",
        "        # Calculate fully connected layer output that yields our class vector\n",
        "        out = self.fc(out)\n",
        "\n",
        "        return out, h_t\n",
        "\n",
        "    def init_hidden(self, batch_size=1):\n",
        "        ''' Initializes hidden state '''\n",
        "        # This is a convenience function so that we can initialize a hidden\n",
        "        # state to zero when we start prediction on a sequence. Every further\n",
        "        # step will then depend on the previous hidden state.\n",
        "\n",
        "        # Create two new tensors with sizes batch_size x n_hidden,\n",
        "        # initialized to zero for hidden the RNN's hidden state.\n",
        "        weight = next(self.parameters()).data\n",
        "        h_t = weight.new(batch_size, self.hidden_sz).zero_().to(device)\n",
        "\n",
        "        return h_t\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UhNyXgWuFK2T"
      },
      "source": [
        "The only thing missing is our training loop. It will look very similar to everything we have previously written, with two main differences:\n",
        "\n",
        "1. Our model is now also dependent on the hidden state and thus takes it as input and returns it as an additional output.\n",
        "2. Because we are using a recurrent neural network we will need to give our \"loss.backward()\" a \"retain_graph=True\" flag in order for it to log the history and be able to compute the backpropagation through time"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LSrhA4lR2EaP"
      },
      "source": [
        "# Declaring the train method\n",
        "def train(model, data, device, optimizer, criterion, epochs=10, batch_size=10,\n",
        "          seq_length=50, clip=5):\n",
        "    model.train()\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        # initialize first hidden states with zeros\n",
        "        h = model.init_hidden(batch_size)\n",
        "\n",
        "        for x, y in get_batches(data, batch_size, seq_length):\n",
        "            # One-hot encode our data, make them torch tensors & cast to device\n",
        "            x = one_hot_encode(x, model.n_chars)\n",
        "            inputs, targets = torch.from_numpy(x), torch.from_numpy(y)\n",
        "            inputs, targets = inputs.to(device), targets.to(device)\n",
        "\n",
        "            # zero accumulated gradients\n",
        "            model.zero_grad()\n",
        "\n",
        "            # get the output and hidden state from the model\n",
        "            output, h = model(inputs, h)\n",
        "\n",
        "            # calculate the loss and perform backprop\n",
        "            # because we have flattened our batch and sequence in the model to\n",
        "            # be able to speed up the connection of the last fully-connected\n",
        "            # layer we now also need to view/flatten our target here\n",
        "            loss = criterion(output, targets.view(batch_size*seq_length).long())\n",
        "            loss.backward(retain_graph=True)\n",
        "\n",
        "            # we use an additional trick of clipping gradients to avoid\n",
        "            # exploding gradients, which is a prominent problem in RNNs, just\n",
        "            # as the opposite problem of vanishing gradients.\n",
        "            nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
        "            optimizer.step()\n",
        "\n",
        "\n",
        "        print(\"Epoch: {}/{}:\".format(epoch + 1, epochs),\n",
        "              \"Loss: {:.4f}:\".format(loss.item()))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FFKl2jWi2GeC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ee78f929-cdf5-49c0-8c08-312a9656a754"
      },
      "source": [
        "# Define the model\n",
        "n_hidden=512\n",
        "model = RNN(chars, device, n_hidden).to(device)\n",
        "\n",
        "# Hyperparameters\n",
        "batch_size = 128\n",
        "seq_length = 100\n",
        "epochs = 300 # start with 50 or similar if you are debugging\n",
        "# train much longer if you want good results\n",
        "\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# train the model\n",
        "history = train(model, encoded, device, optimizer, criterion, epochs=epochs,\n",
        "      batch_size=batch_size, seq_length=seq_length)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 1/300: Loss: 3.4828:\n",
            "Epoch: 2/300: Loss: 3.1949:\n",
            "Epoch: 3/300: Loss: 3.1616:\n",
            "Epoch: 4/300: Loss: 3.1238:\n",
            "Epoch: 5/300: Loss: 3.1390:\n",
            "Epoch: 6/300: Loss: 3.0565:\n",
            "Epoch: 7/300: Loss: 3.0160:\n",
            "Epoch: 8/300: Loss: 2.9772:\n",
            "Epoch: 9/300: Loss: 2.9296:\n",
            "Epoch: 10/300: Loss: 2.8798:\n",
            "Epoch: 11/300: Loss: 2.8275:\n",
            "Epoch: 12/300: Loss: 2.7846:\n",
            "Epoch: 13/300: Loss: 2.9272:\n",
            "Epoch: 14/300: Loss: 2.8463:\n",
            "Epoch: 15/300: Loss: 2.8144:\n",
            "Epoch: 16/300: Loss: 2.8016:\n",
            "Epoch: 17/300: Loss: 2.6904:\n",
            "Epoch: 18/300: Loss: 2.6564:\n",
            "Epoch: 19/300: Loss: 2.6123:\n",
            "Epoch: 20/300: Loss: 2.5739:\n",
            "Epoch: 21/300: Loss: 2.5434:\n",
            "Epoch: 22/300: Loss: 2.5112:\n",
            "Epoch: 23/300: Loss: 2.4883:\n",
            "Epoch: 24/300: Loss: 2.4624:\n",
            "Epoch: 25/300: Loss: 2.4492:\n",
            "Epoch: 26/300: Loss: 2.4214:\n",
            "Epoch: 27/300: Loss: 2.4012:\n",
            "Epoch: 28/300: Loss: 2.3945:\n",
            "Epoch: 29/300: Loss: 2.3772:\n",
            "Epoch: 30/300: Loss: 2.3611:\n",
            "Epoch: 31/300: Loss: 2.3407:\n",
            "Epoch: 32/300: Loss: 2.3220:\n",
            "Epoch: 33/300: Loss: 2.3068:\n",
            "Epoch: 34/300: Loss: 2.2958:\n",
            "Epoch: 35/300: Loss: 2.2839:\n",
            "Epoch: 36/300: Loss: 2.2669:\n",
            "Epoch: 37/300: Loss: 2.2594:\n",
            "Epoch: 38/300: Loss: 2.2536:\n",
            "Epoch: 39/300: Loss: 2.2328:\n",
            "Epoch: 40/300: Loss: 2.2293:\n",
            "Epoch: 41/300: Loss: 2.2246:\n",
            "Epoch: 42/300: Loss: 2.2177:\n",
            "Epoch: 43/300: Loss: 2.2042:\n",
            "Epoch: 44/300: Loss: 2.3762:\n",
            "Epoch: 45/300: Loss: 2.2638:\n",
            "Epoch: 46/300: Loss: 2.2199:\n",
            "Epoch: 47/300: Loss: 2.2066:\n",
            "Epoch: 48/300: Loss: 2.1910:\n",
            "Epoch: 49/300: Loss: 2.1863:\n",
            "Epoch: 50/300: Loss: 2.1666:\n",
            "Epoch: 51/300: Loss: 2.1576:\n",
            "Epoch: 52/300: Loss: 2.1548:\n",
            "Epoch: 53/300: Loss: 2.1458:\n",
            "Epoch: 54/300: Loss: 2.1441:\n",
            "Epoch: 55/300: Loss: 2.1427:\n",
            "Epoch: 56/300: Loss: 2.1360:\n",
            "Epoch: 57/300: Loss: 2.1285:\n",
            "Epoch: 58/300: Loss: 2.2565:\n",
            "Epoch: 59/300: Loss: 2.1367:\n",
            "Epoch: 60/300: Loss: 2.1343:\n",
            "Epoch: 61/300: Loss: 2.1240:\n",
            "Epoch: 62/300: Loss: 2.1133:\n",
            "Epoch: 63/300: Loss: 2.1096:\n",
            "Epoch: 64/300: Loss: 2.1000:\n",
            "Epoch: 65/300: Loss: 2.0999:\n",
            "Epoch: 66/300: Loss: 2.0978:\n",
            "Epoch: 67/300: Loss: 2.0861:\n",
            "Epoch: 68/300: Loss: 2.0910:\n",
            "Epoch: 69/300: Loss: 2.1044:\n",
            "Epoch: 70/300: Loss: 2.0948:\n",
            "Epoch: 71/300: Loss: 2.0983:\n",
            "Epoch: 72/300: Loss: 2.0768:\n",
            "Epoch: 73/300: Loss: 2.0753:\n",
            "Epoch: 74/300: Loss: 2.0716:\n",
            "Epoch: 75/300: Loss: 2.0621:\n",
            "Epoch: 76/300: Loss: 2.0558:\n",
            "Epoch: 77/300: Loss: 2.0497:\n",
            "Epoch: 78/300: Loss: 2.0614:\n",
            "Epoch: 79/300: Loss: 2.0455:\n",
            "Epoch: 80/300: Loss: 2.0485:\n",
            "Epoch: 81/300: Loss: 2.0323:\n",
            "Epoch: 82/300: Loss: 2.0353:\n",
            "Epoch: 83/300: Loss: 2.0422:\n",
            "Epoch: 84/300: Loss: 2.0288:\n",
            "Epoch: 85/300: Loss: 2.0291:\n",
            "Epoch: 86/300: Loss: 2.0274:\n",
            "Epoch: 87/300: Loss: 2.0079:\n",
            "Epoch: 88/300: Loss: 2.0030:\n",
            "Epoch: 89/300: Loss: 1.9993:\n",
            "Epoch: 90/300: Loss: 2.0547:\n",
            "Epoch: 91/300: Loss: 2.0207:\n",
            "Epoch: 92/300: Loss: 2.0211:\n",
            "Epoch: 93/300: Loss: 2.0069:\n",
            "Epoch: 94/300: Loss: 1.9871:\n",
            "Epoch: 95/300: Loss: 1.9843:\n",
            "Epoch: 96/300: Loss: 1.9831:\n",
            "Epoch: 97/300: Loss: 1.9671:\n",
            "Epoch: 98/300: Loss: 1.9730:\n",
            "Epoch: 99/300: Loss: 1.9580:\n",
            "Epoch: 100/300: Loss: 1.9638:\n",
            "Epoch: 101/300: Loss: 1.9611:\n",
            "Epoch: 102/300: Loss: 1.9539:\n",
            "Epoch: 103/300: Loss: 1.9537:\n",
            "Epoch: 104/300: Loss: 1.9485:\n",
            "Epoch: 105/300: Loss: 1.9214:\n",
            "Epoch: 106/300: Loss: 1.9374:\n",
            "Epoch: 107/300: Loss: 1.9369:\n",
            "Epoch: 108/300: Loss: 1.9458:\n",
            "Epoch: 109/300: Loss: 1.9244:\n",
            "Epoch: 110/300: Loss: 1.9212:\n",
            "Epoch: 111/300: Loss: 1.9217:\n",
            "Epoch: 112/300: Loss: 1.9180:\n",
            "Epoch: 113/300: Loss: 1.9090:\n",
            "Epoch: 114/300: Loss: 1.8956:\n",
            "Epoch: 115/300: Loss: 1.8870:\n",
            "Epoch: 116/300: Loss: 1.8829:\n",
            "Epoch: 117/300: Loss: 1.8913:\n",
            "Epoch: 118/300: Loss: 1.8854:\n",
            "Epoch: 119/300: Loss: 1.8920:\n",
            "Epoch: 120/300: Loss: 1.8843:\n",
            "Epoch: 121/300: Loss: 1.8866:\n",
            "Epoch: 122/300: Loss: 1.8767:\n",
            "Epoch: 123/300: Loss: 1.8760:\n",
            "Epoch: 124/300: Loss: 1.8611:\n",
            "Epoch: 125/300: Loss: 1.8570:\n",
            "Epoch: 126/300: Loss: 1.8402:\n",
            "Epoch: 127/300: Loss: 1.8475:\n",
            "Epoch: 128/300: Loss: 1.8442:\n",
            "Epoch: 129/300: Loss: 1.8329:\n",
            "Epoch: 130/300: Loss: 1.8247:\n",
            "Epoch: 131/300: Loss: 1.8294:\n",
            "Epoch: 132/300: Loss: 1.8136:\n",
            "Epoch: 133/300: Loss: 1.8154:\n",
            "Epoch: 134/300: Loss: 1.8225:\n",
            "Epoch: 135/300: Loss: 1.8371:\n",
            "Epoch: 136/300: Loss: 1.8177:\n",
            "Epoch: 137/300: Loss: 1.8391:\n",
            "Epoch: 138/300: Loss: 1.8083:\n",
            "Epoch: 139/300: Loss: 1.8019:\n",
            "Epoch: 140/300: Loss: 1.7946:\n",
            "Epoch: 141/300: Loss: 1.7842:\n",
            "Epoch: 142/300: Loss: 1.7717:\n",
            "Epoch: 143/300: Loss: 1.7984:\n",
            "Epoch: 144/300: Loss: 1.7587:\n",
            "Epoch: 145/300: Loss: 1.7758:\n",
            "Epoch: 146/300: Loss: 1.7831:\n",
            "Epoch: 147/300: Loss: 1.7721:\n",
            "Epoch: 148/300: Loss: 1.7680:\n",
            "Epoch: 149/300: Loss: 1.7739:\n",
            "Epoch: 150/300: Loss: 1.7687:\n",
            "Epoch: 151/300: Loss: 1.7691:\n",
            "Epoch: 152/300: Loss: 1.7844:\n",
            "Epoch: 153/300: Loss: 1.7236:\n",
            "Epoch: 154/300: Loss: 1.7436:\n",
            "Epoch: 155/300: Loss: 1.7234:\n",
            "Epoch: 156/300: Loss: 1.7421:\n",
            "Epoch: 157/300: Loss: 1.7364:\n",
            "Epoch: 158/300: Loss: 1.7322:\n",
            "Epoch: 159/300: Loss: 1.7311:\n",
            "Epoch: 160/300: Loss: 1.7412:\n",
            "Epoch: 161/300: Loss: 1.7217:\n",
            "Epoch: 162/300: Loss: 1.7215:\n",
            "Epoch: 163/300: Loss: 1.6973:\n",
            "Epoch: 164/300: Loss: 1.6895:\n",
            "Epoch: 165/300: Loss: 1.7069:\n",
            "Epoch: 166/300: Loss: 1.6836:\n",
            "Epoch: 167/300: Loss: 1.6714:\n",
            "Epoch: 168/300: Loss: 1.6688:\n",
            "Epoch: 169/300: Loss: 1.6638:\n",
            "Epoch: 170/300: Loss: 1.6662:\n",
            "Epoch: 171/300: Loss: 1.6503:\n",
            "Epoch: 172/300: Loss: 1.6328:\n",
            "Epoch: 173/300: Loss: 1.6549:\n",
            "Epoch: 174/300: Loss: 1.6269:\n",
            "Epoch: 175/300: Loss: 1.6363:\n",
            "Epoch: 176/300: Loss: 1.6461:\n",
            "Epoch: 177/300: Loss: 1.6627:\n",
            "Epoch: 178/300: Loss: 1.6382:\n",
            "Epoch: 179/300: Loss: 1.6624:\n",
            "Epoch: 180/300: Loss: 1.6368:\n",
            "Epoch: 181/300: Loss: 1.6215:\n",
            "Epoch: 182/300: Loss: 1.6032:\n",
            "Epoch: 183/300: Loss: 1.6043:\n",
            "Epoch: 184/300: Loss: 1.6100:\n",
            "Epoch: 185/300: Loss: 1.6289:\n",
            "Epoch: 186/300: Loss: 1.6135:\n",
            "Epoch: 187/300: Loss: 1.5915:\n",
            "Epoch: 188/300: Loss: 1.6068:\n",
            "Epoch: 189/300: Loss: 1.5778:\n",
            "Epoch: 190/300: Loss: 1.5969:\n",
            "Epoch: 191/300: Loss: 1.5756:\n",
            "Epoch: 192/300: Loss: 1.5947:\n",
            "Epoch: 193/300: Loss: 1.5828:\n",
            "Epoch: 194/300: Loss: 1.5951:\n",
            "Epoch: 195/300: Loss: 1.5733:\n",
            "Epoch: 196/300: Loss: 1.5703:\n",
            "Epoch: 197/300: Loss: 1.5536:\n",
            "Epoch: 198/300: Loss: 1.5690:\n",
            "Epoch: 199/300: Loss: 1.5666:\n",
            "Epoch: 200/300: Loss: 1.5677:\n",
            "Epoch: 201/300: Loss: 1.5627:\n",
            "Epoch: 202/300: Loss: 1.5693:\n",
            "Epoch: 203/300: Loss: 1.5569:\n",
            "Epoch: 204/300: Loss: 1.5496:\n",
            "Epoch: 205/300: Loss: 1.5433:\n",
            "Epoch: 206/300: Loss: 1.5585:\n",
            "Epoch: 207/300: Loss: 1.5402:\n",
            "Epoch: 208/300: Loss: 1.5430:\n",
            "Epoch: 209/300: Loss: 1.5504:\n",
            "Epoch: 210/300: Loss: 1.5426:\n",
            "Epoch: 211/300: Loss: 1.5169:\n",
            "Epoch: 212/300: Loss: 1.5365:\n",
            "Epoch: 213/300: Loss: 1.5330:\n",
            "Epoch: 214/300: Loss: 1.5208:\n",
            "Epoch: 215/300: Loss: 1.5222:\n",
            "Epoch: 216/300: Loss: 1.5157:\n",
            "Epoch: 217/300: Loss: 1.5123:\n",
            "Epoch: 218/300: Loss: 1.5249:\n",
            "Epoch: 219/300: Loss: 1.5070:\n",
            "Epoch: 220/300: Loss: 1.5103:\n",
            "Epoch: 221/300: Loss: 1.5135:\n",
            "Epoch: 222/300: Loss: 1.5027:\n",
            "Epoch: 223/300: Loss: 1.4872:\n",
            "Epoch: 224/300: Loss: 1.5032:\n",
            "Epoch: 225/300: Loss: 1.5151:\n",
            "Epoch: 226/300: Loss: 1.5128:\n",
            "Epoch: 227/300: Loss: 1.5150:\n",
            "Epoch: 228/300: Loss: 1.4958:\n",
            "Epoch: 229/300: Loss: 1.4805:\n",
            "Epoch: 230/300: Loss: 1.4828:\n",
            "Epoch: 231/300: Loss: 1.4722:\n",
            "Epoch: 232/300: Loss: 1.4627:\n",
            "Epoch: 233/300: Loss: 1.4671:\n",
            "Epoch: 234/300: Loss: 1.4543:\n",
            "Epoch: 235/300: Loss: 1.4468:\n",
            "Epoch: 236/300: Loss: 1.4549:\n",
            "Epoch: 237/300: Loss: 1.4743:\n",
            "Epoch: 238/300: Loss: 1.4469:\n",
            "Epoch: 239/300: Loss: 1.4457:\n",
            "Epoch: 240/300: Loss: 1.4331:\n",
            "Epoch: 241/300: Loss: 1.4345:\n",
            "Epoch: 242/300: Loss: 1.4199:\n",
            "Epoch: 243/300: Loss: 1.4433:\n",
            "Epoch: 244/300: Loss: 1.4435:\n",
            "Epoch: 245/300: Loss: 1.4310:\n",
            "Epoch: 246/300: Loss: 1.4084:\n",
            "Epoch: 247/300: Loss: 1.4272:\n",
            "Epoch: 248/300: Loss: 1.4125:\n",
            "Epoch: 249/300: Loss: 1.4208:\n",
            "Epoch: 250/300: Loss: 1.4140:\n",
            "Epoch: 251/300: Loss: 1.4297:\n",
            "Epoch: 252/300: Loss: 1.3948:\n",
            "Epoch: 253/300: Loss: 1.4080:\n",
            "Epoch: 254/300: Loss: 1.3978:\n",
            "Epoch: 255/300: Loss: 1.3847:\n",
            "Epoch: 256/300: Loss: 1.4080:\n",
            "Epoch: 257/300: Loss: 1.4122:\n",
            "Epoch: 258/300: Loss: 1.3884:\n",
            "Epoch: 259/300: Loss: 1.3773:\n",
            "Epoch: 260/300: Loss: 1.3526:\n",
            "Epoch: 261/300: Loss: 1.3987:\n",
            "Epoch: 262/300: Loss: 1.3888:\n",
            "Epoch: 263/300: Loss: 1.3971:\n",
            "Epoch: 264/300: Loss: 1.3882:\n",
            "Epoch: 265/300: Loss: 1.4004:\n",
            "Epoch: 266/300: Loss: 1.4055:\n",
            "Epoch: 267/300: Loss: 1.4019:\n",
            "Epoch: 268/300: Loss: 1.4031:\n",
            "Epoch: 269/300: Loss: 1.4300:\n",
            "Epoch: 270/300: Loss: 1.3753:\n",
            "Epoch: 271/300: Loss: 1.3715:\n",
            "Epoch: 272/300: Loss: 1.3437:\n",
            "Epoch: 273/300: Loss: 1.3504:\n",
            "Epoch: 274/300: Loss: 1.3469:\n",
            "Epoch: 275/300: Loss: 1.3264:\n",
            "Epoch: 276/300: Loss: 1.3524:\n",
            "Epoch: 277/300: Loss: 1.3446:\n",
            "Epoch: 278/300: Loss: 1.3517:\n",
            "Epoch: 279/300: Loss: 1.3836:\n",
            "Epoch: 280/300: Loss: 1.3852:\n",
            "Epoch: 281/300: Loss: 1.3672:\n",
            "Epoch: 282/300: Loss: 1.3533:\n",
            "Epoch: 283/300: Loss: 1.3335:\n",
            "Epoch: 284/300: Loss: 1.3226:\n",
            "Epoch: 285/300: Loss: 1.3208:\n",
            "Epoch: 286/300: Loss: 1.3280:\n",
            "Epoch: 287/300: Loss: 1.3568:\n",
            "Epoch: 288/300: Loss: 1.3514:\n",
            "Epoch: 289/300: Loss: 1.3498:\n",
            "Epoch: 290/300: Loss: 1.3444:\n",
            "Epoch: 291/300: Loss: 1.3159:\n",
            "Epoch: 292/300: Loss: 1.3189:\n",
            "Epoch: 293/300: Loss: 1.3114:\n",
            "Epoch: 294/300: Loss: 1.3132:\n",
            "Epoch: 295/300: Loss: 1.3008:\n",
            "Epoch: 296/300: Loss: 1.3378:\n",
            "Epoch: 297/300: Loss: 1.3201:\n",
            "Epoch: 298/300: Loss: 1.3352:\n",
            "Epoch: 299/300: Loss: 1.2974:\n",
            "Epoch: 300/300: Loss: 1.3315:\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vIndlrMSGMmZ"
      },
      "source": [
        "You should observe the loss sinking consistently. In fact you can observe that the model training hasn't fully converged yet. If you feel like you want to spend the time later to see how\n",
        "well you can get your RNN to perform, try training it for longer/until convergence.\n",
        "\n",
        "Once we have trained the model it will be interesting to use it for prediction. To generate new content we would like to feed in an initial sequence or even just a single character and see what the model generates for the rest of the sequence conditioned on our input.\n",
        "\n",
        "Let us write the logic for that:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A53V0aKj2IfT"
      },
      "source": [
        "def predict(model, char, device, h=None, top_k=5):\n",
        "        ''' Given a character & hidden state, predict the next character.\n",
        "            Returns the predicted character and the hidden state.\n",
        "        '''\n",
        "\n",
        "        # tensor inputs\n",
        "        x = np.array([[model.char2int[char]]])\n",
        "        x = one_hot_encode(x, model.n_chars)\n",
        "        inputs = torch.from_numpy(x).to(device)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            # get the output of the model\n",
        "            out, h = model(inputs, h)\n",
        "\n",
        "            # get the character probabilities\n",
        "            # move to cpu for further processing with numpy etc.\n",
        "            p = F.softmax(out, dim=1).data.cpu()\n",
        "\n",
        "            # get the top characters with highest likelihood\n",
        "            p, top_ch = p.topk(top_k)\n",
        "            top_ch = top_ch.numpy().squeeze()\n",
        "\n",
        "            # select the likely next character with some element of randomness\n",
        "            # for more variability\n",
        "            p = p.numpy().squeeze()\n",
        "            char = np.random.choice(top_ch, p=p/p.sum())\n",
        "\n",
        "        # return the encoded value of the predicted char and the hidden state\n",
        "        return model.int2char[char], h"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wHAMT5xTlYY0"
      },
      "source": [
        "def sample(model, size, device, prime='A', top_k=None):\n",
        "    # method to generate new text based on a \"prime\"/initial sequence.\n",
        "    # Basically, the outer loop convenience function that calls the above\n",
        "    # defined predict method.\n",
        "    model.eval() # eval mode\n",
        "\n",
        "    # Calculate model for the initial prime characters\n",
        "    chars = [ch for ch in prime]\n",
        "    with torch.no_grad():\n",
        "        # initialize hidden with 0 in the beginning. Set our batch size to 1\n",
        "        # as we wish to generate one sequence only.\n",
        "        h = model.init_hidden(batch_size=1)\n",
        "        for ch in prime:\n",
        "            char, h = predict(model, ch, device, h=h, top_k=top_k)\n",
        "\n",
        "        # append the characters to the sequence\n",
        "        chars.append(char)\n",
        "\n",
        "        # Now pass in the previous/last character and get a new one\n",
        "        # Repeat this process for the desired length of the sequence to be\n",
        "        # generated\n",
        "        for ii in range(size):\n",
        "            char, h = predict(model, chars[-1], device, h=h, top_k=top_k)\n",
        "            chars.append(char)\n",
        "\n",
        "    return ''.join(chars)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1wgYQs7FH2wo"
      },
      "source": [
        "### Generating poems\n",
        "\n",
        "We are now set to call our sample method with our trained model, a prime sequence and a desired sequence length to be generated."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HClso847laed",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e17aa3f6-6b61-4d87-ca05-cdd8a4a30b0d"
      },
      "source": [
        "print(sample(model, 1000, device, prime='A', top_k=5))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "And doth sull corlt the our share shise:\n",
            "Shear forthar ant moth and ca in thes with dor tout hach mones, and of memy, an thea ly me tace meait ound, thou thuinst youl henou baiget in eat areas boise\n",
            "My hevin soo tillf cin, atr llover forme,\n",
            "Which thot dight unde theus rnat love tious mad tise fore to ghaas,\n",
            "And thes wor houn to bast ter aid thit mound trigevenye pines than whal aspaeno nsorthou ghoud same whals, an  he mown then my homrtsen mey e aut toree, thed fe thouthed, atl male leateres mesh woles, have tome soul aro theisst your theas ow trueniry crepees intheir maledor oll ghtor,\n",
            "Whend,\n",
            "Whin I tingea thee, yndeas youn wenctisurgat to seef in wirte, ant mant roug,\n",
            "Tore's shinke sa peasende groce is artond.\n",
            "But be thye commy treawling my bele dong toug time,e so fought lach I dofrest rimes il pat in thos dedisss deate tree lore,\n",
            "Thes of the horren thet. bntiles ant leerer io tee inthet we lines this shisen bee es rat bo ant or lisge wastirghtse wath his patises on meve mase the sha\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rjkj103bICMs"
      },
      "source": [
        "We can see that our RNN typically starts out correctly and sometimes is able to generate correct words but quickly goes on to generate junk as there is no long term dependencies.\n",
        "\n",
        "We will now implement a PyTorch LSTM to see how to improve upon this."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NPG6v0M7IVOB"
      },
      "source": [
        "## Long Short Term Memory (LSTM)\n",
        "\n",
        "Let's take our recurrent neural network class that we have defined above and replace the simple RNN cell with one or even multiple stacked LSTM cells.\n",
        "\n",
        "If you want to go for the challenge you can try implementing this by hand similarly to the RNN cell we have defined. However, if you don't want to go through the tour-de-force exercise, you can go on ahead and use PyTorch's \"nn.LSTM()\" convenience method: https://pytorch.org/docs/stable/nn.html#torch.nn.LSTM\n",
        "\n",
        "You can try using a stack of 2 LSTM hidden layers to simply replace the RNN cell."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1dIQmxMEDs41"
      },
      "source": [
        "class LSTM(nn.Module):\n",
        "\n",
        "    def __init__(self, chars, device, n_hidden=256, n_layers=2, drop_prob=0.5):\n",
        "        super().__init__()\n",
        "\n",
        "        self.device = device\n",
        "\n",
        "        self.drop_prob = drop_prob\n",
        "        self.n_layers = n_layers\n",
        "        self.n_hidden = n_hidden\n",
        "\n",
        "        # creating character dictionaries\n",
        "        # we already have this code on the top, but giving it to our model\n",
        "        # will be convenient for doing predictions later\n",
        "        # i.e. doing conversions from text to integers to one-hot & vice-versa\n",
        "        self.n_chars = len(chars)\n",
        "        self.int2char = dict(enumerate(chars))\n",
        "        self.char2int = {ch: ii for ii, ch in self.int2char.items()}\n",
        "\n",
        "        # define the LSTM\n",
        "        # we no longer need to care about wieght initialization as PyTorch\n",
        "        # will handle this for us now.\n",
        "        # When defining PyTorch's nn.LSTM() set \"batch_first=True\" to assign\n",
        "        # the batch size to the first dimension (instead of the sequence) to\n",
        "        # stay consistent with our RNN implementation and re-use our code.\n",
        "        self.lstm = nn.LSTM(self.n_chars, n_hidden, n_layers,\n",
        "                            dropout=drop_prob, batch_first=True)\n",
        "\n",
        "        # define a dropout layer\n",
        "        self.dropout = nn.Dropout(drop_prob)\n",
        "\n",
        "        # define the final, fully-connected output layer\n",
        "        self.fc = nn.Linear(n_hidden, self.n_chars)\n",
        "\n",
        "    def forward(self, x, hidden):\n",
        "        ''' Forward pass through the network.\n",
        "            The inputs are x, and the hidden & cell state in a tuple. '''\n",
        "\n",
        "        # get the outputs and the new hidden states from the LSTM.\n",
        "        # Note that the hidden variable now is a tuple of hidden and cell state\n",
        "        # in contrast to the RNN that just had the hidden state.\n",
        "        # Because we are using the PyTorch LSTM we do not need to implement\n",
        "        # the loop anymore as the sequence will be handled internally.\n",
        "        r_output, hidden = self.lstm(x, hidden)\n",
        "\n",
        "        # pass through a dropout layer\n",
        "        out = self.dropout(r_output)\n",
        "\n",
        "        # Stack up the LSTM outputs using view so that we can process the last\n",
        "        # layer in parallel\n",
        "        out = out.contiguous().view(-1, self.n_hidden)\n",
        "\n",
        "        # Calculate fully connected layer output that yields our class vector\n",
        "        out = self.fc(out)\n",
        "\n",
        "        return out, hidden\n",
        "\n",
        "    def init_hidden(self, batch_size=1):\n",
        "        ''' Initializes hidden state '''\n",
        "        # This is a convenience function so that we can initialize the hidden\n",
        "        # states to zero when we start prediction on a sequence. Every further\n",
        "        # step will then depend on the previous hidden states (c and h).\n",
        "\n",
        "        # Create a tuple of two new tensors with sizes\n",
        "        # n_layers x batch_size x n_hidden, initialized to zero for the\n",
        "        # LSTM hidden and cell states.\n",
        "        weight = next(self.parameters()).data\n",
        "\n",
        "        hidden = (weight.new(self.n_layers, batch_size, self.n_hidden).zero_().to(device),\n",
        "                  weight.new(self.n_layers, batch_size, self.n_hidden).zero_().to(device))\n",
        "\n",
        "        return hidden"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GL9kCT6TK2QT"
      },
      "source": [
        "We can use the exact same code to train our newly defined LSTM model. Let's try with the same amount of hidden units and 2 LSTM cells."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cW70FQwT2g_h",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "eaf709ff-a333-4b39-e51b-7a5804dab29d"
      },
      "source": [
        "# Define the model\n",
        "n_hidden=512\n",
        "n_layers=2\n",
        "\n",
        "model = LSTM(chars, device, n_hidden, n_layers).to(device)\n",
        "\n",
        "# Declaring the hyperparameters\n",
        "batch_size = 128\n",
        "seq_length = 100\n",
        "epochs = 300 # start with 50 or similar if you are debugging\n",
        "# train much longer if you want good results\n",
        "\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# train the model\n",
        "train(model, encoded, device, optimizer, criterion, epochs=epochs,\n",
        "      batch_size=batch_size, seq_length=seq_length)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 1/300: Loss: 3.2958:\n",
            "Epoch: 2/300: Loss: 3.1619:\n",
            "Epoch: 3/300: Loss: 3.1385:\n",
            "Epoch: 4/300: Loss: 3.1233:\n",
            "Epoch: 5/300: Loss: 3.1181:\n",
            "Epoch: 6/300: Loss: 3.1096:\n",
            "Epoch: 7/300: Loss: 3.1078:\n",
            "Epoch: 8/300: Loss: 3.1050:\n",
            "Epoch: 9/300: Loss: 3.1020:\n",
            "Epoch: 10/300: Loss: 3.0960:\n",
            "Epoch: 11/300: Loss: 3.0874:\n",
            "Epoch: 12/300: Loss: 3.0717:\n",
            "Epoch: 13/300: Loss: 3.0437:\n",
            "Epoch: 14/300: Loss: 2.9839:\n",
            "Epoch: 15/300: Loss: 2.9060:\n",
            "Epoch: 16/300: Loss: 2.8361:\n",
            "Epoch: 17/300: Loss: 2.7283:\n",
            "Epoch: 18/300: Loss: 2.6460:\n",
            "Epoch: 19/300: Loss: 2.5754:\n",
            "Epoch: 20/300: Loss: 2.5015:\n",
            "Epoch: 21/300: Loss: 2.4515:\n",
            "Epoch: 22/300: Loss: 2.4095:\n",
            "Epoch: 23/300: Loss: 2.3696:\n",
            "Epoch: 24/300: Loss: 2.3640:\n",
            "Epoch: 25/300: Loss: 2.3401:\n",
            "Epoch: 26/300: Loss: 2.3380:\n",
            "Epoch: 27/300: Loss: 2.3055:\n",
            "Epoch: 28/300: Loss: 2.2827:\n",
            "Epoch: 29/300: Loss: 2.2547:\n",
            "Epoch: 30/300: Loss: 2.2373:\n",
            "Epoch: 31/300: Loss: 2.2166:\n",
            "Epoch: 32/300: Loss: 2.1934:\n",
            "Epoch: 33/300: Loss: 2.1742:\n",
            "Epoch: 34/300: Loss: 2.1565:\n",
            "Epoch: 35/300: Loss: 2.1491:\n",
            "Epoch: 36/300: Loss: 2.1208:\n",
            "Epoch: 37/300: Loss: 2.1111:\n",
            "Epoch: 38/300: Loss: 2.0828:\n",
            "Epoch: 39/300: Loss: 2.0793:\n",
            "Epoch: 40/300: Loss: 2.0526:\n",
            "Epoch: 41/300: Loss: 2.0338:\n",
            "Epoch: 42/300: Loss: 2.0186:\n",
            "Epoch: 43/300: Loss: 2.0062:\n",
            "Epoch: 44/300: Loss: 1.9992:\n",
            "Epoch: 45/300: Loss: 1.9943:\n",
            "Epoch: 46/300: Loss: 1.9654:\n",
            "Epoch: 47/300: Loss: 1.9598:\n",
            "Epoch: 48/300: Loss: 1.9469:\n",
            "Epoch: 49/300: Loss: 1.9286:\n",
            "Epoch: 50/300: Loss: 1.9240:\n",
            "Epoch: 51/300: Loss: 1.9118:\n",
            "Epoch: 52/300: Loss: 1.9075:\n",
            "Epoch: 53/300: Loss: 1.8847:\n",
            "Epoch: 54/300: Loss: 1.8769:\n",
            "Epoch: 55/300: Loss: 1.8626:\n",
            "Epoch: 56/300: Loss: 1.8591:\n",
            "Epoch: 57/300: Loss: 1.8435:\n",
            "Epoch: 58/300: Loss: 1.8385:\n",
            "Epoch: 59/300: Loss: 1.8221:\n",
            "Epoch: 60/300: Loss: 1.8206:\n",
            "Epoch: 61/300: Loss: 1.8135:\n",
            "Epoch: 62/300: Loss: 1.8101:\n",
            "Epoch: 63/300: Loss: 1.8075:\n",
            "Epoch: 64/300: Loss: 1.7918:\n",
            "Epoch: 65/300: Loss: 1.7870:\n",
            "Epoch: 66/300: Loss: 1.7730:\n",
            "Epoch: 67/300: Loss: 1.7698:\n",
            "Epoch: 68/300: Loss: 1.7554:\n",
            "Epoch: 69/300: Loss: 1.7472:\n",
            "Epoch: 70/300: Loss: 1.7484:\n",
            "Epoch: 71/300: Loss: 1.7335:\n",
            "Epoch: 72/300: Loss: 1.7336:\n",
            "Epoch: 73/300: Loss: 1.7252:\n",
            "Epoch: 74/300: Loss: 1.7093:\n",
            "Epoch: 75/300: Loss: 1.7010:\n",
            "Epoch: 76/300: Loss: 1.6919:\n",
            "Epoch: 77/300: Loss: 1.6926:\n",
            "Epoch: 78/300: Loss: 1.6775:\n",
            "Epoch: 79/300: Loss: 1.6713:\n",
            "Epoch: 80/300: Loss: 1.6764:\n",
            "Epoch: 81/300: Loss: 1.6719:\n",
            "Epoch: 82/300: Loss: 1.6558:\n",
            "Epoch: 83/300: Loss: 1.6499:\n",
            "Epoch: 84/300: Loss: 1.6478:\n",
            "Epoch: 85/300: Loss: 1.6216:\n",
            "Epoch: 86/300: Loss: 1.6217:\n",
            "Epoch: 87/300: Loss: 1.6106:\n",
            "Epoch: 88/300: Loss: 1.6168:\n",
            "Epoch: 89/300: Loss: 1.6076:\n",
            "Epoch: 90/300: Loss: 1.6053:\n",
            "Epoch: 91/300: Loss: 1.6033:\n",
            "Epoch: 92/300: Loss: 1.5841:\n",
            "Epoch: 93/300: Loss: 1.5756:\n",
            "Epoch: 94/300: Loss: 1.5648:\n",
            "Epoch: 95/300: Loss: 1.5600:\n",
            "Epoch: 96/300: Loss: 1.5587:\n",
            "Epoch: 97/300: Loss: 1.5435:\n",
            "Epoch: 98/300: Loss: 1.5366:\n",
            "Epoch: 99/300: Loss: 1.5314:\n",
            "Epoch: 100/300: Loss: 1.5328:\n",
            "Epoch: 101/300: Loss: 1.5234:\n",
            "Epoch: 102/300: Loss: 1.5169:\n",
            "Epoch: 103/300: Loss: 1.5087:\n",
            "Epoch: 104/300: Loss: 1.4995:\n",
            "Epoch: 105/300: Loss: 1.4920:\n",
            "Epoch: 106/300: Loss: 1.4790:\n",
            "Epoch: 107/300: Loss: 1.4816:\n",
            "Epoch: 108/300: Loss: 1.4598:\n",
            "Epoch: 109/300: Loss: 1.4567:\n",
            "Epoch: 110/300: Loss: 1.4555:\n",
            "Epoch: 111/300: Loss: 1.4416:\n",
            "Epoch: 112/300: Loss: 1.4296:\n",
            "Epoch: 113/300: Loss: 1.4137:\n",
            "Epoch: 114/300: Loss: 1.4090:\n",
            "Epoch: 115/300: Loss: 1.3979:\n",
            "Epoch: 116/300: Loss: 1.3939:\n",
            "Epoch: 117/300: Loss: 1.3863:\n",
            "Epoch: 118/300: Loss: 1.3706:\n",
            "Epoch: 119/300: Loss: 1.3582:\n",
            "Epoch: 120/300: Loss: 1.3597:\n",
            "Epoch: 121/300: Loss: 1.3344:\n",
            "Epoch: 122/300: Loss: 1.3358:\n",
            "Epoch: 123/300: Loss: 1.3127:\n",
            "Epoch: 124/300: Loss: 1.3246:\n",
            "Epoch: 125/300: Loss: 1.3148:\n",
            "Epoch: 126/300: Loss: 1.3056:\n",
            "Epoch: 127/300: Loss: 1.2979:\n",
            "Epoch: 128/300: Loss: 1.2893:\n",
            "Epoch: 129/300: Loss: 1.2702:\n",
            "Epoch: 130/300: Loss: 1.2685:\n",
            "Epoch: 131/300: Loss: 1.2558:\n",
            "Epoch: 132/300: Loss: 1.2495:\n",
            "Epoch: 133/300: Loss: 1.2284:\n",
            "Epoch: 134/300: Loss: 1.2207:\n",
            "Epoch: 135/300: Loss: 1.2079:\n",
            "Epoch: 136/300: Loss: 1.1858:\n",
            "Epoch: 137/300: Loss: 1.1734:\n",
            "Epoch: 138/300: Loss: 1.1631:\n",
            "Epoch: 139/300: Loss: 1.1608:\n",
            "Epoch: 140/300: Loss: 1.1544:\n",
            "Epoch: 141/300: Loss: 1.1497:\n",
            "Epoch: 142/300: Loss: 1.1186:\n",
            "Epoch: 143/300: Loss: 1.1209:\n",
            "Epoch: 144/300: Loss: 1.1177:\n",
            "Epoch: 145/300: Loss: 1.1094:\n",
            "Epoch: 146/300: Loss: 1.1079:\n",
            "Epoch: 147/300: Loss: 1.0957:\n",
            "Epoch: 148/300: Loss: 1.0780:\n",
            "Epoch: 149/300: Loss: 1.0655:\n",
            "Epoch: 150/300: Loss: 1.0714:\n",
            "Epoch: 151/300: Loss: 1.0437:\n",
            "Epoch: 152/300: Loss: 1.0406:\n",
            "Epoch: 153/300: Loss: 1.0284:\n",
            "Epoch: 154/300: Loss: 1.0191:\n",
            "Epoch: 155/300: Loss: 1.0102:\n",
            "Epoch: 156/300: Loss: 0.9884:\n",
            "Epoch: 157/300: Loss: 0.9786:\n",
            "Epoch: 158/300: Loss: 0.9671:\n",
            "Epoch: 159/300: Loss: 0.9487:\n",
            "Epoch: 160/300: Loss: 0.9197:\n",
            "Epoch: 161/300: Loss: 0.9179:\n",
            "Epoch: 162/300: Loss: 0.8978:\n",
            "Epoch: 163/300: Loss: 0.9078:\n",
            "Epoch: 164/300: Loss: 0.8892:\n",
            "Epoch: 165/300: Loss: 0.8739:\n",
            "Epoch: 166/300: Loss: 0.8638:\n",
            "Epoch: 167/300: Loss: 0.8491:\n",
            "Epoch: 168/300: Loss: 0.8473:\n",
            "Epoch: 169/300: Loss: 0.8448:\n",
            "Epoch: 170/300: Loss: 0.8238:\n",
            "Epoch: 171/300: Loss: 0.8379:\n",
            "Epoch: 172/300: Loss: 0.8151:\n",
            "Epoch: 173/300: Loss: 0.8077:\n",
            "Epoch: 174/300: Loss: 0.7795:\n",
            "Epoch: 175/300: Loss: 0.7905:\n",
            "Epoch: 176/300: Loss: 0.7647:\n",
            "Epoch: 177/300: Loss: 0.7456:\n",
            "Epoch: 178/300: Loss: 0.7578:\n",
            "Epoch: 179/300: Loss: 0.7425:\n",
            "Epoch: 180/300: Loss: 0.7269:\n",
            "Epoch: 181/300: Loss: 0.7303:\n",
            "Epoch: 182/300: Loss: 0.7293:\n",
            "Epoch: 183/300: Loss: 0.7096:\n",
            "Epoch: 184/300: Loss: 0.7088:\n",
            "Epoch: 185/300: Loss: 0.6874:\n",
            "Epoch: 186/300: Loss: 0.6811:\n",
            "Epoch: 187/300: Loss: 0.6640:\n",
            "Epoch: 188/300: Loss: 0.6729:\n",
            "Epoch: 189/300: Loss: 0.6733:\n",
            "Epoch: 190/300: Loss: 0.6490:\n",
            "Epoch: 191/300: Loss: 0.6374:\n",
            "Epoch: 192/300: Loss: 0.6304:\n",
            "Epoch: 193/300: Loss: 0.6209:\n",
            "Epoch: 194/300: Loss: 0.6074:\n",
            "Epoch: 195/300: Loss: 0.6056:\n",
            "Epoch: 196/300: Loss: 0.5902:\n",
            "Epoch: 197/300: Loss: 0.5594:\n",
            "Epoch: 198/300: Loss: 0.5586:\n",
            "Epoch: 199/300: Loss: 0.5354:\n",
            "Epoch: 200/300: Loss: 0.5253:\n",
            "Epoch: 201/300: Loss: 0.5125:\n",
            "Epoch: 202/300: Loss: 0.5180:\n",
            "Epoch: 203/300: Loss: 0.5272:\n",
            "Epoch: 204/300: Loss: 0.5079:\n",
            "Epoch: 205/300: Loss: 0.5189:\n",
            "Epoch: 206/300: Loss: 0.5100:\n",
            "Epoch: 207/300: Loss: 0.5082:\n",
            "Epoch: 208/300: Loss: 0.4879:\n",
            "Epoch: 209/300: Loss: 0.4807:\n",
            "Epoch: 210/300: Loss: 0.4728:\n",
            "Epoch: 211/300: Loss: 0.4700:\n",
            "Epoch: 212/300: Loss: 0.4561:\n",
            "Epoch: 213/300: Loss: 0.4589:\n",
            "Epoch: 214/300: Loss: 0.4688:\n",
            "Epoch: 215/300: Loss: 0.4512:\n",
            "Epoch: 216/300: Loss: 0.4282:\n",
            "Epoch: 217/300: Loss: 0.4404:\n",
            "Epoch: 218/300: Loss: 0.4528:\n",
            "Epoch: 219/300: Loss: 0.4333:\n",
            "Epoch: 220/300: Loss: 0.4182:\n",
            "Epoch: 221/300: Loss: 0.4106:\n",
            "Epoch: 222/300: Loss: 0.3975:\n",
            "Epoch: 223/300: Loss: 0.3934:\n",
            "Epoch: 224/300: Loss: 0.3821:\n",
            "Epoch: 225/300: Loss: 0.3809:\n",
            "Epoch: 226/300: Loss: 0.3751:\n",
            "Epoch: 227/300: Loss: 0.3654:\n",
            "Epoch: 228/300: Loss: 0.3677:\n",
            "Epoch: 229/300: Loss: 0.3700:\n",
            "Epoch: 230/300: Loss: 0.3616:\n",
            "Epoch: 231/300: Loss: 0.3603:\n",
            "Epoch: 232/300: Loss: 0.3615:\n",
            "Epoch: 233/300: Loss: 0.3514:\n",
            "Epoch: 234/300: Loss: 0.3355:\n",
            "Epoch: 235/300: Loss: 0.3298:\n",
            "Epoch: 236/300: Loss: 0.3276:\n",
            "Epoch: 237/300: Loss: 0.3093:\n",
            "Epoch: 238/300: Loss: 0.3052:\n",
            "Epoch: 239/300: Loss: 0.3171:\n",
            "Epoch: 240/300: Loss: 0.3029:\n",
            "Epoch: 241/300: Loss: 0.3220:\n",
            "Epoch: 242/300: Loss: 0.3000:\n",
            "Epoch: 243/300: Loss: 0.3117:\n",
            "Epoch: 244/300: Loss: 0.2875:\n",
            "Epoch: 245/300: Loss: 0.2820:\n",
            "Epoch: 246/300: Loss: 0.2769:\n",
            "Epoch: 247/300: Loss: 0.2692:\n",
            "Epoch: 248/300: Loss: 0.2615:\n",
            "Epoch: 249/300: Loss: 0.2577:\n",
            "Epoch: 250/300: Loss: 0.2666:\n",
            "Epoch: 251/300: Loss: 0.2542:\n",
            "Epoch: 252/300: Loss: 0.2577:\n",
            "Epoch: 253/300: Loss: 0.2568:\n",
            "Epoch: 254/300: Loss: 0.2517:\n",
            "Epoch: 255/300: Loss: 0.2560:\n",
            "Epoch: 256/300: Loss: 0.2436:\n",
            "Epoch: 257/300: Loss: 0.2361:\n",
            "Epoch: 258/300: Loss: 0.2436:\n",
            "Epoch: 259/300: Loss: 0.2500:\n",
            "Epoch: 260/300: Loss: 0.2375:\n",
            "Epoch: 261/300: Loss: 0.2492:\n",
            "Epoch: 262/300: Loss: 0.2248:\n",
            "Epoch: 263/300: Loss: 0.2274:\n",
            "Epoch: 264/300: Loss: 0.2264:\n",
            "Epoch: 265/300: Loss: 0.2238:\n",
            "Epoch: 266/300: Loss: 0.2155:\n",
            "Epoch: 267/300: Loss: 0.2148:\n",
            "Epoch: 268/300: Loss: 0.2052:\n",
            "Epoch: 269/300: Loss: 0.2097:\n",
            "Epoch: 270/300: Loss: 0.2099:\n",
            "Epoch: 271/300: Loss: 0.2016:\n",
            "Epoch: 272/300: Loss: 0.2055:\n",
            "Epoch: 273/300: Loss: 0.1942:\n",
            "Epoch: 274/300: Loss: 0.2020:\n",
            "Epoch: 275/300: Loss: 0.1990:\n",
            "Epoch: 276/300: Loss: 0.2007:\n",
            "Epoch: 277/300: Loss: 0.1914:\n",
            "Epoch: 278/300: Loss: 0.1835:\n",
            "Epoch: 279/300: Loss: 0.1910:\n",
            "Epoch: 280/300: Loss: 0.1819:\n",
            "Epoch: 281/300: Loss: 0.1810:\n",
            "Epoch: 282/300: Loss: 0.1876:\n",
            "Epoch: 283/300: Loss: 0.1719:\n",
            "Epoch: 284/300: Loss: 0.1771:\n",
            "Epoch: 285/300: Loss: 0.1731:\n",
            "Epoch: 286/300: Loss: 0.1665:\n",
            "Epoch: 287/300: Loss: 0.1680:\n",
            "Epoch: 288/300: Loss: 0.1743:\n",
            "Epoch: 289/300: Loss: 0.1680:\n",
            "Epoch: 290/300: Loss: 0.1679:\n",
            "Epoch: 291/300: Loss: 0.1691:\n",
            "Epoch: 292/300: Loss: 0.1735:\n",
            "Epoch: 293/300: Loss: 0.1657:\n",
            "Epoch: 294/300: Loss: 0.1598:\n",
            "Epoch: 295/300: Loss: 0.1605:\n",
            "Epoch: 296/300: Loss: 0.1517:\n",
            "Epoch: 297/300: Loss: 0.1572:\n",
            "Epoch: 298/300: Loss: 0.1539:\n",
            "Epoch: 299/300: Loss: 0.1565:\n",
            "Epoch: 300/300: Loss: 0.1447:\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OTznlDdJK81w"
      },
      "source": [
        "We can observe that our model is able to achieve a much lower loss than before with our simple RNN implementation. This should now also be reflected when we generate/sample new sonnets.\n",
        "\n",
        "Again, you can observe that the loss still continues to improve, even after 300 epochs. For the best results, try training the model longer."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m4pmGQ5J2lmJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4732fa22-5a3e-4895-e4f2-bdee91b1f0d9"
      },
      "source": [
        "# Generating new text\n",
        "print(sample(model, 1000, device, prime='A', top_k=5))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Agring so theis\n",
            "To the side merie, by thy cuntan's chime,\n",
            "And state thou thoughts, whethin the eust from Tome\n",
            "Whach shind of winter's with a baint,\n",
            "As natger's ching thinge leasted to my heart.\n",
            "\n",
            "As tise  im stain, in every faring swaet.\n",
            "Thy bid what tenders the birot and west,\n",
            "Thou canst not men tis golding cenfore;\n",
            "And finthes is not my mear fear trush.\n",
            "To to amo s extobre away, 'er beeto that,\n",
            "Thou art chesfing is say 'stile to thee,\n",
            "The rogh of sech-my some barners come\n",
            "And farit cansting, leaves urig thee,\n",
            "Therefore be it never how with king?\n",
            "The swort to my gondent, grown did thre bark\n",
            "And most home inter my doy night;\n",
            "So love that mine eye is me thing to thine showed,\n",
            "Then thou consubsed and thou brestered sing:\n",
            "The ease chand mine own deasher'd hoor so self in tous.\n",
            "\n",
            "So if that will, thou thou, thou chust he with gove,\n",
            "To then that whil they thoughts' bost how in his doth\n",
            "Mans love in his by altion quile compare\n",
            "To proud for more fild with you do thy eye.\n",
            "\n",
            "Why lovest in my here, b\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AXT-5Y-rLRKt"
      },
      "source": [
        "Arguably there is still discrepancy to the original Shakespeare texts in our just generated examples. However, if we compare this output to our RNN's output, we can see that the LSTM is able to achieve much more consistency in generating proper words as well as sometimes portions of sentences that have improved in terms of grammar. There now seems to be more overall structure."
      ]
    }
  ]
}